{"cells":[{"cell_type":"markdown","metadata":{"id":"rX8mhOLljYeM"},"source":["## RSM8421: AI and Deep Learning Tools - Tutorial 2"]},{"cell_type":"markdown","metadata":{"id":"_nXOokVH0xDV"},"source":["Welcome to the second tutorial! In Tutorial 1 we built a dense neural network for the MNIST digits dataset. In this tutorial, we pivot to natural language processing and create a movie-review sentiment analysis pipeline. You will see how to move from raw text files to neural networks that reason about language.\n","\n","By the end of this notebook you will be able to:\n","- explain how preprocessing choices (cleaning, vocabulary size, sequence length) shape downstream models,\n","- implement and train a convolutional neural network (CNN),\n","- implement and train a bidirectional recurrent neural network (RNN), and\n","- compare model behavior with shared metrics, plots, and qualitative predictions.\n","\n","Run each cell as you read the explanations. Before executing the code, try to predict what will happen and relate it back to the theory covered in class.\n"]},{"cell_type":"markdown","metadata":{"id":"04QgGZc9bF5D"},"source":["### Learning roadmap\n","\n","1. **Part 1 – Data preparation, word embeddings, and vectorization:** download the raw IMDB dataset, clean the text, adapt a `TextVectorization` layer, and visualize the resulting vocabulary.\n","2. **Part 2 – CNN sentiment classifier:** build a fast 1-D convolutional model that looks for n-gram level evidence of positive or negative tone.\n","3. **Part 3 – RNN sentiment classifier:** replace convolutions with a bidirectional LSTM that reads the review sequentially and captures order-sensitive cues.\n","4. **Part 4 – Comparisons:** evaluate both models on the same splits, plot their training curves, and inspect side-by-side predictions.\n"]},{"cell_type":"markdown","metadata":{"id":"nnrWf3PCEzXL"},"source":["## Part 1 – Data preparation, word embeddings, and vectorization\n"]},{"cell_type":"markdown","metadata":{"id":"MpsGtV500xDX"},"source":["### 1.1 Configure the environment and reproducibility switches\n","\n","Check https://www.tensorflow.org/install for guidelines on installing TensorFlow. We begin by importing TensorFlow along with a couple of helper libraries for numerical computing and visualization. TensorFlow ships with the high-level Keras API that we will rely on to define layers quickly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13834,"status":"ok","timestamp":1763673770363,"user":{"displayName":"Keqing Wang","userId":"07255036791946745799"},"user_tz":300},"id":"ipfpGsmB0xDX","outputId":"480a4c7d-31b3-451a-a53c-9f00e71bcb49"},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow version: 2.19.0\n","Seeds set for reproducibility.\n"]}],"source":["# Text Cleaning and Processing Libraries- Install liabary& Recheck the dataset and the packages\n","import re\n","import string\n","\n","# System and File Handling Libraries\n","import tarfile\n","import urllib.request\n","import shutil\n","from pathlib import Path\n","\n","# Counting labels\n","from collections import Counter\n","\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","plt.style.use(\"seaborn-v0_8\")\n","plt.rcParams[\"figure.figsize\"] = (8, 5)\n","print(\"TensorFlow version:\", tf.__version__)\n","\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","print('Seeds set for reproducibility.')\n"]},{"cell_type":"markdown","metadata":{"id":"FyU7Hji90xDY"},"source":["### 1.2 Download IMDB reviews dataset\n"]},{"cell_type":"markdown","metadata":{"id":"It141y5L0xDY"},"source":["We revisit the classic Stanford IMDB dataset that ships as raw text files. There are 25,000 labeled reviews for training and 25,000 for testing, each tagged as positive (1) or negative (0).\n","\n","We first download and extract the archive (skip the download if you already have it locally). We then remove the unlabeled reviews so that we focus entirely on supervised learning."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1763673770387,"user":{"displayName":"Keqing Wang","userId":"07255036791946745799"},"user_tz":300},"id":"e94a8a0cd0d741e490fca4536005f49c","outputId":"d8d2e086-5ebd-4670-cf08-f55755b8cb98"},"outputs":[{"name":"stdout","output_type":"stream","text":["aclImdb directory already present. Skipping download and extraction.\n"]}],"source":["dataset_url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n","archive_path = Path(\"aclImdb_v1.tar.gz\")\n","data_dir = Path(\"aclImdb\")\n","\n","if not data_dir.exists():\n","    if not archive_path.exists():\n","        print(\"Downloading IMDB archive (~80MB)...\")\n","        urllib.request.urlretrieve(dataset_url, archive_path)\n","        print(\"Download complete.\")\n","    else:\n","        print(\"Archive already exists. Skipping download.\")\n","    print(\"Extracting archive...\")\n","    with tarfile.open(archive_path, mode=\"r:gz\") as tar:\n","        tar.extractall()\n","    print(\"Extraction finished.\")\n","else:\n","    print(\"aclImdb directory already present. Skipping download and extraction.\")\n","\n","unsup_dir = data_dir / \"train\" / \"unsup\"\n","if unsup_dir.exists():\n","    shutil.rmtree(unsup_dir) # Removing the dicrectional tree,\n","    print(\"Removed unlabeled training reviews (unsup).\") # Removed the unlabeled data\n"]},{"cell_type":"markdown","metadata":{"id":"0bb0a98d66804f6ea1c4a24fb9c8f0ab"},"source":["### 1.3 Turn directories into batched tf.data datasets\n","\n","We now build training, validation, and test datasets with tf.data and examine their class balance. Keras ships with text_dataset_from_directory, which reads the folder structure (`neg/` and `pos/`) and emits tf.data. dataset objects of (text, label) pairs. We keep 20% of the training split for validation so that the CNN and RNN later on see identical splits.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3862,"status":"ok","timestamp":1763673774252,"user":{"displayName":"Keqing Wang","userId":"07255036791946745799"},"user_tz":300},"id":"b279f5d0e9874f8aa0fd69afc91c317f","outputId":"1f9a9c93-3d5e-45be-e849-04896eac0658"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 25000 files belonging to 2 classes.\n","Using 20000 files for training.\n","Found 25000 files belonging to 2 classes.\n","Using 5000 files for validation.\n","Found 25000 files belonging to 2 classes.\n","Class names: ['neg', 'pos']\n","Train batches per epoch: tf.Tensor(625, shape=(), dtype=int64)\n","Validation batches per epoch: tf.Tensor(157, shape=(), dtype=int64)\n","Test batches per epoch: tf.Tensor(782, shape=(), dtype=int64)\n"]}],"source":["\n","raw_train_ds = tf.keras.utils.text_dataset_from_directory( # Using keras and tenser flows\n","    data_dir / \"train\",\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=42,\n",")# The tranining and validation are using the extratly same set\n","\n","\n","raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n","    data_dir / \"train\",\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=42,\n",")\n","\n","raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n","    data_dir / \"test\",\n",")\n","\n","print(\"Class names:\", raw_train_ds.class_names)\n","print(\"Train batches per epoch:\", tf.data.experimental.cardinality(raw_train_ds))\n","print(\"Validation batches per epoch:\", tf.data.experimental.cardinality(raw_val_ds))\n","print(\"Test batches per epoch:\", tf.data.experimental.cardinality(raw_test_ds))\n","# Why is the 625? Mini-batch size is 625; the image number 20k, the batch szie by default is 32 20k/32=625"]},{"cell_type":"markdown","metadata":{"id":"d33422dd33a041fba08e3225cf68e347"},"source":["### 1.4 Inspect label balance and review lengths\n","\n","We now visualize review lengths to select the sequence_length .\n","\n","Before vectorizing anything, it helps to know whether the dataset is balanced and how long the reviews are. The histogram below uses 4,000 training reviews to visualize word counts and overlays the chosen sequence_length to show how much truncation/padding we will perform.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1tNQMy0KPILVlUPLUDjRvOCth9fMYZg9i"},"id":"b19e09cf59254b86b9662383e0ab1669","outputId":"7f81c055-9667-4c13-89af-c3ab12e26d99"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Splite the paragrpjahocs to avioid the review is too long\n","sequence_length = 500\n","sample_size = 4000 # From 20K total sample pick 4000 training sample\n","\n","label_counts = Counter()\n","review_lengths = []\n","\n","\n","for text, label in raw_train_ds.unbatch().take(sample_size):\n","    label_counts[int(label.numpy())] += 1\n","    review_lengths.append(len(text.numpy().decode(\"utf-8\").split()))\n","    print(text)\n","    print(text.numpy().decode(\"utf-8\"))\n","    #sampletext = raw_train_ds.unbatch().take(1)\n","#print(text.numpy().decode(\"utf-8\"))\n","\n","print(\"Label distribution in first\", sample_size, \"training reviews:\")\n","for idx, count in sorted(label_counts.items()):\n","    print(f\"  {raw_train_ds.class_names[idx]}: {count}\")\n","\n","lengths = np.array(review_lengths)\n","coverage = (lengths <= sequence_length).mean() * 100\n","p90 = np.percentile(lengths, 90)\n","print(f\"Mean tokens: {lengths.mean():.1f} | Median: {np.median(lengths):.1f} | 90th percentile: {p90:.1f}\")\n","print(f\"Chosen sequence_length covers {coverage:.1f}% of these reviews without truncation.\")\n","\n","plt.figure(figsize=(9, 4))\n","plt.hist(lengths, bins=40, color=\"#4c72b0\", edgecolor=\"white\")\n","plt.axvline(sequence_length, color=\"#dd8452\", linestyle=\"--\", label=\"sequence_length\")\n","plt.title(\"Distribution of token counts in sampled training reviews\")\n","plt.xlabel(\"Number of word tokens\")\n","plt.ylabel(\"Frequency\")\n","plt.legend()\n","plt.tight_layout()\n","\n","# Convert the sentences capitalletters, gmails, commas, pouctuation\n"]},{"cell_type":"markdown","metadata":{"id":"0e7895aaa5f44a919a8e30145ee7b13c"},"source":["### 1.5 Clean raw text with a custom standardization function\n","\n","We now clean the text, adapt a TextVectorization layer, and look at the learned vocabulary. The IMDB files contain HTML break tags (`<br />`) and punctuation that we do not want the model to treat as standalone tokens. We therefore lowercase everything, strip HTML, and remove punctuation before tokenization.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4be7fb14b5fc439f9170b12f31f4bbee","outputId":"12947ed6-e187-4145-c152-852f8a8fd9bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["!\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n","Original text: <br />Amazing film!!! Emotional AND funny.\n","After custom_standardization:  amazing film emotional and funny\n"]}],"source":["def custom_standardization(input_data):\n","    lowercase = tf.strings.lower(input_data)\n","    no_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n","    cleaned = tf.strings.regex_replace(\n","        no_html,\n","        f\"[{re.escape(string.punctuation)}]\",\n","        \"\"\n","    )\n","    return cleaned\n","print(re.escape(string.punctuation))\n","# Inside the string puntuation: !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n","sample_text = tf.constant([\"<br />Amazing film!!! Emotional AND funny.\"], dtype=tf.string)\n","print(\"Original text:\", sample_text.numpy()[0].decode(\"utf-8\"))\n","print(\"After custom_standardization:\", custom_standardization(sample_text).numpy()[0].decode(\"utf-8\"))\n"]},{"cell_type":"markdown","metadata":{"id":"330b8cf3a9d74032b950f62bd89602f9"},"source":["TextVectorization learns the most frequent tokens from the training set and converts each cleaned review into a list of integer IDs. We cap the vocabulary at max_features = 20,000 tokens and pad or truncate every sequence to exactly sequence_length = 500 so later models can work with dense tensors.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6775c84e2f08435ca2daeaeaa58767e4","outputId":"3a9c589d-1fb1-47ee-909b-ab758840d1b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Adapting TextVectorization layer (this scans the training text once)...\n","Vocabulary size: 20000\n","Top 10 tokens: ['', '[UNK]', np.str_('the'), np.str_('and'), np.str_('a'), np.str_('of'), np.str_('to'), np.str_('is'), np.str_('in'), np.str_('it')]\n","Sample tokens 200-210: [np.str_('series'), np.str_('gets'), np.str_('without'), np.str_('come'), np.str_('always'), np.str_('right'), np.str_('times'), np.str_('isnt'), np.str_('saw'), np.str_('long')]\n"]}],"source":["max_features = 20000 #??\n","# translate text into numerics integers\n","vectorize_layer = tf.keras.layers.TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=max_features, # the maximum vocabarays 20k words, OOV out of order vocabulary (vacaburay really meaningless ouside of usage)\n","    output_mode=\"int\", #numerics integers\n","    output_sequence_length=sequence_length,\n",")\n","\n","print(\"Adapting TextVectorization layer (this scans the training text once)...\")\n","text_ds = raw_train_ds.map(lambda x, y: x)\n","vectorize_layer.adapt(text_ds)\n","\n","vocab = vectorize_layer.get_vocabulary() # Vectorize the vocabuary, numerical vector embedding the characteristics,\n","print(\"Vocabulary size:\", len(vocab))\n","print(\"Top 10 tokens:\", vocab[:10])\n","print(\"Sample tokens 200-210:\", vocab[200:210])\n"]},{"cell_type":"markdown","metadata":{"id":"5a0dbc3491b847ebb01e938c9515a8a5"},"source":["### 1.6 See what vectorization produces for a sample review\n","\n","To keep the process tangible, take a short sentence, vectorize it, and decode the first few non-zero IDs back to their tokens.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1e626005d9ff46c8bfb7e8c2cbaaa72a","outputId":"faffe432-c98b-4b8b-b479-8a9bb93c0ae8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original review: This movie was AMAZING!!! <br /> Loved it from start to finish.\n","First 15 token IDs: [  11   17   13  463  446    9   35  368    6 1323]\n","Decoded tokens: [np.str_('this'), np.str_('movie'), np.str_('was'), np.str_('amazing'), np.str_('loved'), np.str_('it'), np.str_('from'), np.str_('start'), np.str_('to'), np.str_('finish')]\n"]}],"source":["sample_review = tf.constant([\n","    \"This movie was AMAZING!!! <br /> Loved it from start to finish.\"\n","], dtype=tf.string)\n","vectorized = vectorize_layer(sample_review).numpy()[0]\n","non_zero_ids = vectorized[vectorized != 0][:15] # Extract from dictionay the ids non-zero\n","reconstructed_tokens = [vocab[idx] for idx in non_zero_ids] # Vocabary dictory the id integer\n","\n","print(\"Original review:\"\n",", sample_review.numpy()[0].decode(\"utf-8\"))\n","print(\"First 15 token IDs:\", non_zero_ids)\n","print(\"Decoded tokens:\", reconstructed_tokens)\n"]},{"cell_type":"markdown","metadata":{"id":"92bc7af8a0214f8583c67c550f246611"},"source":["### 1.7 Prepare fast input pipelines for later models\n","\n","We wrap the vectorization logic inside `tf.data` pipelines so future models receive ready-to-use tensors. Caching and prefetching overlap CPU preprocessing with GPU training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"d6c012719e4a4916a68e9458df7e9b53","outputId":"d7fc501f-a9cf-4dd6-d17c-dd150a9d86f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vectorized batch shape: (32, 500)\n","Labels shape: (32,)\n","First review token IDs (first 20 positions): [   2    1    7    4   73 7844 4199  200  188 1397 2232    3    4   49\n","  111    8  695    1   71  821]\n"]}],"source":["def vectorize_text(text, label):\n","    text = tf.expand_dims(text, -1)\n","    return vectorize_layer(text), label\n","\n","train_ds = raw_train_ds.map(vectorize_text)\n","val_ds = raw_val_ds.map(vectorize_text)\n","test_ds = raw_test_ds.map(vectorize_text)\n","# Cache through the total epoachs, keep all the vectors every run the epoches and batches when training CNNs\n","# prefetch 32 fetch (Mini batch) through the first epoch send 32 from CPU to GPU, multiple circulation and allowance on queue from CPU sent batches\n","train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","example_batch = next(iter(train_ds.take(1)))\n","example_features, example_labels = example_batch\n","print(\"Vectorized batch shape:\", example_features.shape)\n","print(\"Labels shape:\", example_labels.shape)\n","print(\"First review token IDs (first 20 positions):\", example_features[0][:20].numpy())\n"]},{"cell_type":"markdown","metadata":{"id":"b3f49380b1c94427b3e187abfe8667ba"},"source":["### 1.8 Embedding intuition: from sparse tokens to dense geometry\n","\n","An embedding layer learns a dense vector for every token ID so that similar reviews receive similar representations. The heatmaps below show 12 token positions from three sample sentences mapped into a 128-dimensional space (colors represent the initial random values that future training will adjust).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"866dd55a0e4d4b279d64fa4480954f74"},"outputs":[],"source":["\n","embedding_dim = 128 # ?????Embedding Dimensions 128 , the size of image individual\n","num_positions = 10\n","embedding_demo_layer = tf.keras.layers.Embedding(\n","    input_dim=max_features,\n","    output_dim=embedding_dim,\n","    name=\"demo_embedding\"\n",")\n","\n","demo_sentences = tf.constant([\n","    \"Absolutely wonderful storytelling and performances\",\n","    \"I fell asleep halfway through because it was so dull\",\n","    \"Great acting but the script felt uneven and rushed\"\n","], dtype=tf.string)\n","\n","demo_tokens = vectorize_layer(demo_sentences)[:, :num_positions]\n","demo_vectors = embedding_demo_layer(demo_tokens)\n","demo_texts = [text.decode(\"utf-8\") for text in demo_sentences.numpy()]\n","demo_token_labels = []\n","for row in demo_tokens.numpy():\n","    tokens = [vocab[idx] if idx != 0 else \"<pad>\" for idx in row]\n","    demo_token_labels.append(tokens)\n","\n","fig, axes = plt.subplots(\n","    len(demo_texts),\n","    1,\n","    figsize=(10, 2.5 * len(demo_texts)),\n","    constrained_layout=True,\n",")\n","if not isinstance(axes, (list, np.ndarray)):\n","    axes = [axes]\n","\n","for ax, text, token_labels, vectors in zip(axes, demo_texts, demo_token_labels, demo_vectors):\n","    mat = vectors.numpy().T\n","    im = ax.imshow(mat, aspect=\"auto\", cmap=\"viridis\")\n","    ax.set_title(text)\n","    ax.set_ylabel(\"Embedding dim\")\n","    ax.set_xticks(range(num_positions))\n","    ax.set_xticklabels(token_labels, rotation=45, ha=\"right\", fontsize=9)\n","    ax.set_yticks([])\n","\n","cbar = fig.colorbar(im, ax=axes, fraction=0.03, pad=0.04)\n","cbar.set_label(\"Value before training\")\n","plt.xlabel(\"Token position\")\n","\n","# Zero padding: make the legthen of sentence into same size 500 euqualized and exactly\n","# Add fitted number of zeros into the input two sides"]},{"cell_type":"markdown","metadata":{"id":"aa45d6dfdf164048a1d91bc856c10e0e"},"source":["### 1.10 Function to visualize model training later on\n","\n","We finish Part 1 by defining a small plotting helper that both the CNN and RNN will reuse. Keeping it here highlights that tooling for evaluation is part of the data-preparation story.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0f64894"},"outputs":[],"source":["def plot_history(history, title_prefix=\"Model\"):\n","    acc = history.history.get(\"accuracy\", [])\n","    val_acc = history.history.get(\"val_accuracy\", [])\n","    loss = history.history.get(\"loss\", [])\n","    val_loss = history.history.get(\"val_loss\", [])\n","    epochs_range = range(1, len(acc) + 1)\n","\n","    if acc:\n","        plt.figure()\n","        plt.plot(epochs_range, acc, label=\"Train acc\")\n","        plt.plot(epochs_range, val_acc, label=\"Val acc\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Accuracy\")\n","        plt.title(f\"{title_prefix} Accuracy\")\n","        plt.legend()\n","        plt.tight_layout()\n","\n","    if loss:\n","        plt.figure()\n","        plt.plot(epochs_range, loss, label=\"Train loss\")\n","        plt.plot(epochs_range, val_loss, label=\"Val loss\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Loss\")\n","        plt.title(f\"{title_prefix} Loss\")\n","        plt.legend()\n","        plt.tight_layout()\n"]},{"cell_type":"markdown","metadata":{"id":"c573ceaadf4940a792048bc00d354d61"},"source":["## Part 2 – Convolutional neural network for sentiment\n","\n","With vectorized reviews in hand we can feed them into a convolutional neural network. A 1-D CNN slides filters across the embedded tokens to detect short phrases such as “not good” or “absolutely loved.” We follow the following:\n","\n","1. Embed tokens into dense vectors that are learned jointly with the classifier.\n","2. Apply dropout and stacked Conv1D layers to capture n-gram evidence.\n","3. Use global max pooling to convert variable-length sequences into fixed-size representations.\n","4. Add dense layers and a sigmoid output for binary sentiment.\n","\n","We train for a few epochs, inspect the learning curves, and wrap the model with the TextVectorization layer so it can accept raw strings just like any production API.\n"]},{"cell_type":"markdown","metadata":{"id":"42e12e2d303c428abc9d5706d4ef83b6"},"source":["\n","### 2.1 Connect vectorized reviews to convolution-friendly tensors\n","\n","`train_ds` now yields padded sequences of token IDs with shape `(batch, sequence_length)`. Before building a full CNN it helps to confirm those shapes and see how an embedding layer turns each integer into a dense vector.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1581550df893440390b15f8b2e3345c9"},"outputs":[],"source":["\n","cnn_example_tokens, cnn_example_labels = next(iter(train_ds.take(1)))\n","print(\"Vectorized batch:\", cnn_example_tokens.shape)\n","print(\"Label batch:\", cnn_example_labels.shape)\n","\n","embedding_probe = tf.keras.layers.Embedding(input_dim=max_features, output_dim=embedding_dim)\n","example_embeddings = embedding_probe(cnn_example_tokens[:1])\n","print(\"Embedding output shape for one review:\", example_embeddings.shape)\n","print(\"First token ID:\", int(cnn_example_tokens[0, 0]), \"-> embedding vector (first 5 dims):\", example_embeddings[0, 0, :5].numpy())\n","## Create the"]},{"cell_type":"markdown","metadata":{"id":"0aebdcf9d1f347d9ac6b33f70a47fdbf"},"source":["\n","### 2.2 Build the CNN block by block\n","\n","A 1-D CNN treats the sequence dimension like a spatial axis. We embed tokens, drop out some activations for regularization, stack two convolutional layers to detect meaningful n-grams, pool across time, and finish with dense layers. The `build_cnn_model` helper below keeps these decisions encapsulated so we can reuse the architecture later.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1b883614855542ec80fb765c78793b0a"},"outputs":[],"source":["\n","def build_cnn_model(\n","    vocab_size=max_features,\n","    embed_dim=embedding_dim,\n","    conv_filters=256,\n","    kernel_size=5,\n","    dense_units=256, # Too large overfitting to noises, large capacity,\n","    dropout_rate=0.35,\n","    l2_reg=1e-4,\n","):\n","    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"token_ids\")\n","    x = tf.keras.layers.Embedding(vocab_size, embed_dim, name=\"embedding\")(inputs)\n","    x = tf.keras.layers.SpatialDropout1D(dropout_rate, name=\"dropout_emb\")(x)\n","    reg = tf.keras.regularizers.l2(l2_reg)\n","    x = tf.keras.layers.Conv1D(\n","        filters=conv_filters,\n","        kernel_size=kernel_size,\n","        padding=\"same\",\n","        activation=\"relu\",\n","        kernel_regularizer=reg,\n","        name=\"conv1\",\n","    )(x)\n","    x = tf.keras.layers.BatchNormalization(name=\"bn1\")(x)\n","    x = tf.keras.layers.Conv1D(\n","        filters=conv_filters,\n","        kernel_size=kernel_size,\n","        padding=\"same\",\n","        activation=\"relu\",\n","        kernel_regularizer=reg,\n","        name=\"conv2\",\n","    )(x)\n","    x = tf.keras.layers.BatchNormalization(name=\"bn2\")(x)\n","    x = tf.keras.layers.GlobalMaxPooling1D(name=\"global_max_pool\")(x)\n","    x = tf.keras.layers.Dense(\n","        dense_units,\n","        activation=\"relu\",\n","        kernel_regularizer=reg,\n","        name=\"dense_hidden\",\n","    )(x)\n","    x = tf.keras.layers.Dropout(dropout_rate, name=\"dropout_hidden\")(x)\n","    outputs = tf.keras.layers.Dense(\n","        1,\n","        activation=\"sigmoid\",\n","        kernel_regularizer=reg,\n","        name=\"output\",\n","    )(x)\n","    return tf.keras.Model(inputs, outputs, name=\"cnn_model\")\n","\n","cnn_model = build_cnn_model()\n","cnn_model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"b446424fc639401a8f1dba6dc261fbf1"},"source":["\n","### 2.3 Visualize how convolution windows slide across embeddings\n","\n","Each `Conv1D` kernel looks at `kernel_size` consecutive tokens at a time. The heatmap below feeds one sample review through a toy convolutional layer (random weights) to illustrate how activations are produced across the sequence.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6365ba96e4c34548935683f12a1b8959"},"outputs":[],"source":["conv_window = 7\n","conv_filters = 4\n","sample_tokens = cnn_example_tokens[:1, :80]\n","sample_embeddings = embedding_probe(sample_tokens)\n","conv_demo = tf.keras.layers.Conv1D(filters=conv_filters, kernel_size=conv_window, strides=1, activation=\"relu\")\n","activations = conv_demo(sample_embeddings).numpy()[0].T  # (filters, steps)\n","\n","plt.figure(figsize=(9, 3))\n","plt.imshow(activations, aspect=\"auto\", cmap=\"viridis\")\n","plt.colorbar(label=\"Activation strength\")\n","plt.title(\"Toy convolution activations across one review\")\n","plt.xlabel(\"Sliding window position\")\n","plt.ylabel(\"Filter index\")\n","plt.tight_layout()\n"]},{"cell_type":"markdown","metadata":{"id":"08a64e31423248b19cfd9608e1ed203c"},"source":["\n","### 2.4 Compile the CNN with binary cross-entropy and Adam\n","\n","Sentiment analysis is a binary classification problem. We pair the sigmoid output with binary_crossentropy and monitor accuracy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19b58439b52d4fac9d14cd367c7dcafb"},"outputs":[],"source":["cnn_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n","cnn_model.compile(\n","    loss=\"binary_crossentropy\",\n","    optimizer=cnn_optimizer,\n","    metrics=[\"accuracy\"],\n",")\n","print(\"CNN compiled and ready to train.\")\n"]},{"cell_type":"markdown","metadata":{"id":"df439423ea6449b682a4eada6b36d475"},"source":["\n","### 2.5 Train the CNN\n","\n","We reuse the cached datasets from Part 1, train a bit longer with a reduce-on-plateau schedule, and call plot_history so the curves are adjacent to the training output.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8182f7d3f1a24729ba4be013fd815ba2"},"outputs":[],"source":["train_ds = train_ds.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n","\n","cnn_callbacks = [\n","    tf.keras.callbacks.ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=3,  # less aggressive\n","        min_lr=1e-5,\n","        verbose=1,\n","    ),\n","    tf.keras.callbacks.EarlyStopping(\n","        patience=3,\n","        restore_best_weights=True,\n","        verbose=1,\n","    ),\n","]\n","\n","history_cnn = cnn_model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=10,\n","    callbacks=cnn_callbacks,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"22b677bb"},"source":["\n","### 2.6 Peek at trained CNN embeddings\n","\n","Replot the same sample sentences after CNN training to see how the embedding layer reshapes token vectors once it has learned from the data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34fa9016"},"outputs":[],"source":["\n","cnn_embed_layer = cnn_model.get_layer(\"embedding\")\n","cnn_sentences = tf.constant([\n","    \"Absolutely wonderful storytelling and performances\",\n","    \"I fell asleep halfway through because it was so dull\",\n","    \"Great acting but the script felt uneven and rushed\",\n","], dtype=tf.string)\n","\n","cnn_tokens = vectorize_layer(cnn_sentences)[:, :num_positions]\n","cnn_token_labels = []\n","for row in cnn_tokens.numpy():\n","    tokens = [vocab[idx] if idx != 0 else \"<pad>\" for idx in row]\n","    cnn_token_labels.append(tokens)\n","cnn_vectors = cnn_embed_layer(cnn_tokens)\n","\n","fig, axes = plt.subplots(\n","    len(cnn_sentences),\n","    1,\n","    figsize=(10, 2.5 * len(cnn_sentences)),\n","    constrained_layout=True,\n",")\n","if not isinstance(axes, (list, np.ndarray)):\n","    axes = [axes]\n","\n","for ax, text, token_labels, vectors in zip(axes, cnn_sentences.numpy(), cnn_token_labels, cnn_vectors):\n","    mat = vectors.numpy().T  # (embedding_dim, num_positions)\n","    im = ax.imshow(mat, aspect=\"auto\", cmap=\"viridis\")\n","    ax.set_title(text.decode(\"utf-8\"))\n","    ax.set_ylabel(\"Embedding dim\")\n","    ax.set_xticks(range(num_positions))\n","    ax.set_xticklabels(token_labels, rotation=45, ha=\"right\", fontsize=9)\n","    ax.set_yticks([])\n","\n","cbar = fig.colorbar(im, ax=axes, fraction=0.03, pad=0.04)\n","cbar.set_label(\"CNN embedding value (post-training)\")\n","plt.xlabel(\"Token position\")\n"]},{"cell_type":"markdown","metadata":{"id":"f749d9e32a1048c5906a06d2d9f9f229"},"source":["\n","### 2.7 Evaluate on the held-out test set\n","\n","After training, we compute the final test loss/accuracy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"573083dbd7af4c2c90a97d720bc3a9d0"},"outputs":[],"source":["\n","test_loss_cnn, test_acc_cnn = cnn_model.evaluate(test_ds)\n","print(f\"CNN test loss: {test_loss_cnn:.4f}\")\n","print(f\"CNN test accuracy: {test_acc_cnn:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"7f7ce63dd4e14cf38f0ff42d9b6fcf71"},"source":["\n","### 2.8 Wrap preprocessing + model for raw-string inference\n","\n","In practice we rarely receive pre-vectorized tensors. Wrapping the trained CNN with the TextVectorization layer gives us a single graph that accepts raw strings. The sanity-check predictions double as a demo for how to deploy the model later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"839e52959c8e40cebe3da92b7c884eb1"},"outputs":[],"source":["\n","cnn_inputs_str = tf.keras.Input(shape=(), dtype=tf.string, name=\"text\")\n","cnn_indices = vectorize_layer(cnn_inputs_str)\n","cnn_outputs_str = cnn_model(cnn_indices)\n","\n","cnn_end_to_end = tf.keras.Model(cnn_inputs_str, cnn_outputs_str, name=\"cnn_end_to_end\")\n","cnn_end_to_end.compile(\n","    loss=\"binary_crossentropy\",\n","    optimizer=\"adam\",\n","    metrics=[\"accuracy\"],\n",")\n","\n","sample_texts = [\n","    \"I absolutely loved this movie. The acting was fantastic and the story was touching.\",\n","    \"This was the worst film I have seen in years. Boring and predictable.\",\n","]\n","sample_texts_tensor = tf.constant(sample_texts, dtype=tf.string)\n","cnn_preds = cnn_end_to_end.predict(sample_texts_tensor)\n","for text, p in zip(sample_texts, cnn_preds):\n","    p_val = float(p.item())\n","    print(f\"[CNN] {text!r} -> positive probability = {p_val:.3f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"8d284804fc134374ba526097d3a49ebc"},"source":["## Part 3 – Recurrent neural network for sentiment\n","\n","Convolutions focus on local patterns. Recurrent networks reason about order explicitly by processing one token at a time and carrying information across the sequence. We build a bidirectional LSTM so the model can read the review forward and backward, capture dependencies that span many words, and then pass the final representation through dense layers. The training loop mirrors the CNN so we can compare apples to apples.\n"]},{"cell_type":"markdown","metadata":{"id":"02c05e76b0624934a7910dee075f640e"},"source":["\n","### 3.1 Review sequential inputs and token order\n","\n","Recurrent networks read one token at a time, so order matters. The cell below inspects a vectorized batch, prints the decoded tokens for the first review, and reminds us that padding fills the tail with zeros.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0c3eca5a4dca4aca82ffadcbbc1944a0"},"outputs":[],"source":["\n","rnn_example_tokens, rnn_example_labels = next(iter(train_ds.take(1)))\n","print(\"Vectorized batch shape:\", rnn_example_tokens.shape)\n","print(\"Label batch shape:\", rnn_example_labels.shape)\n","\n","first_review_ids = rnn_example_tokens[0][:20].numpy()\n","decoded_tokens = [vocab[idx] for idx in first_review_ids if idx != 0]\n","print(\"First 20 token IDs:\", first_review_ids)\n","print(\"Decoded tokens (truncated at first pad token):\", decoded_tokens)\n","print(\"Label:\", int(rnn_example_labels[0].numpy()))\n"]},{"cell_type":"markdown","metadata":{"id":"8391c4df0bec4ae8bc73584a85abdd7c"},"source":["\n","### 3.2 Build the bidirectional LSTM sentiment model\n","\n","We embed the input sequence, apply dropout, pass it through a bidirectional LSTM (64 units in each direction), and finish with dense layers. The forward/backward combination lets the network reason about both preceding and following context for every token.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2ed125375f041e6be12127cb4169798"},"outputs":[],"source":["\n","def build_rnn_model(\n","    vocab_size=max_features,\n","    embed_dim=embedding_dim,\n","    lstm_units=128,\n","    dense_units=256,\n","    dropout_rate=0.35,\n","    recurrent_dropout=0.2,\n","    l2_reg=1e-4,\n","):\n","    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"token_ids\")\n","    x = tf.keras.layers.Embedding(vocab_size, embed_dim, name=\"embedding\")(inputs)\n","    x = tf.keras.layers.Dropout(dropout_rate, name=\"dropout_emb\")(x)\n","    reg = tf.keras.regularizers.l2(l2_reg)\n","    x = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(\n","            lstm_units,\n","            return_sequences=False,\n","            dropout=recurrent_dropout,\n","            kernel_regularizer=reg,\n","            recurrent_regularizer=reg,\n","        ),\n","        name=\"bilstm\",\n","    )(x)\n","    x = tf.keras.layers.Dense(\n","        dense_units,\n","        activation=\"relu\",\n","        kernel_regularizer=reg,\n","        name=\"dense_hidden\",\n","    )(x)\n","    x = tf.keras.layers.Dropout(dropout_rate, name=\"dropout_hidden\")(x)\n","    outputs = tf.keras.layers.Dense(\n","        1,\n","        activation=\"sigmoid\",\n","        kernel_regularizer=reg,\n","        name=\"output\",\n","    )(x)\n","    return tf.keras.Model(inputs, outputs, name=\"rnn_model\")\n","\n","rnn_model = build_rnn_model()\n","rnn_model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"16ce212c08924898b503010027fdb9e3"},"source":["\n","### 3.3 Peek at hidden-state dynamics across the sequence\n","\n","To build intuition, we feed one review through a lightweight bidirectional LSTM with `return_sequences=True` and plot the hidden activations along the time axis. Unlike the CNN heatmap, these activations depend on everything the network has seen so far.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"184a320a3ce4422bad42869048c4eea1"},"outputs":[],"source":["\n","demo_length = 60\n","demo_embedding = tf.keras.layers.Embedding(max_features, 32)\n","demo_bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True))\n","\n","demo_tokens = rnn_example_tokens[:1, :demo_length]\n","demo_vectors = demo_embedding(demo_tokens)\n","demo_states = demo_bilstm(demo_vectors).numpy()[0].T  # shape: (2*16, steps)\n","\n","plt.figure(figsize=(10, 3))\n","plt.imshow(demo_states, aspect=\"auto\", cmap=\"coolwarm\")\n","plt.colorbar(label=\"Hidden activation\")\n","plt.title(\"Bidirectional LSTM hidden states across one review\")\n","plt.xlabel(\"Token position\")\n","plt.ylabel(\"Hidden dimension\")\n","plt.tight_layout()\n"]},{"cell_type":"markdown","metadata":{"id":"e1e66c80ec0b44d2b0b7720e0d3a1616"},"source":["\n","### 3.4 Compile the RNN with binary cross-entropy\n","\n","Sentiment is still a binary label, so we reuse the sigmoid + binary_crossentropy combination and monitor accuracy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e91c26f1f20043ef944589f83493aacc"},"outputs":[],"source":["\n","rnn_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n","\n","rnn_model.compile(\n","    loss=\"binary_crossentropy\",\n","    optimizer=rnn_optimizer,\n","    metrics=[\"accuracy\"],\n",")\n","print(\"RNN compiled and ready to train.\")\n"]},{"cell_type":"markdown","metadata":{"id":"aac29ffed6ef4f668354996ff3316075"},"source":["\n","### 3.5 Train the RNN and compare curves to the CNN\n","\n","Training mirrors Part 2 with the same epoch budget and learning-rate schedule as the CNN so the comparison is apples to apples. Plotting the history right after fitting makes it easier to contrast convergence behavior.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b902f9b8da984c058aa508074b3dbcbb"},"outputs":[],"source":["rnn_epochs = 3\n","rnn_callbacks = [\n","    tf.keras.callbacks.ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=1,\n","        min_lr=1e-5,\n","        verbose=1,\n","    ),\n","    tf.keras.callbacks.EarlyStopping(# The Early Stopping is not necessary,\n","        patience=2,\n","        monitor=\"val_loss\",\n","        restore_best_weights=True,\n","        verbose=1,\n","    ),\n","]\n","history_rnn = rnn_model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=rnn_epochs,\n","    callbacks=rnn_callbacks,\n",")\n","\n","plot_history(history_rnn, \"RNN (BiLSTM)\")\n"]},{"cell_type":"markdown","metadata":{"id":"1JY3WrrB0xDg"},"source":["### 3.6 Peek at trained RNN embeddings\n","\n","Replot the same sample sentences after RNN training to compare how the BiLSTM embedding layer arranges token vectors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iL_pnzkk0xDg"},"outputs":[],"source":["\n","rnn_embed_layer = rnn_model.get_layer(\"embedding\")\n","rnn_sentences = tf.constant([\n","    \"Absolutely wonderful storytelling and performances\",\n","    \"I fell asleep halfway through because it was so dull\",\n","    \"Great acting but the script felt uneven and rushed\",\n","], dtype=tf.string)\n","\n","rnn_tokens = vectorize_layer(rnn_sentences)[:, :num_positions]\n","rnn_token_labels = []\n","for row in rnn_tokens.numpy():\n","    tokens = [vocab[idx] if idx != 0 else \"<pad>\" for idx in row]\n","    rnn_token_labels.append(tokens)\n","rnn_vectors = rnn_embed_layer(rnn_tokens)\n","\n","fig, axes = plt.subplots(\n","    len(rnn_sentences),\n","    1,\n","    figsize=(10, 2.5 * len(rnn_sentences)),\n","    constrained_layout=True,\n",")\n","if not isinstance(axes, (list, np.ndarray)):\n","    axes = [axes]\n","\n","for ax, text, token_labels, vectors in zip(axes, rnn_sentences.numpy(), rnn_token_labels, rnn_vectors):\n","    mat = vectors.numpy().T  # (embedding_dim, num_positions)\n","    im = ax.imshow(mat, aspect=\"auto\", cmap=\"viridis\")\n","    ax.set_title(text.decode(\"utf-8\"))\n","    ax.set_ylabel(\"Embedding dim\")\n","    ax.set_xticks(range(num_positions))\n","    ax.set_xticklabels(token_labels, rotation=45, ha=\"right\", fontsize=9)\n","    ax.set_yticks([])\n","\n","cbar = fig.colorbar(im, ax=axes, fraction=0.03, pad=0.04)\n","cbar.set_label(\"RNN embedding value (post-training)\")\n","plt.xlabel(\"Token position\")\n"]},{"cell_type":"markdown","metadata":{"id":"7b394e16da6f44959ca08e54c4781196"},"source":["\n","### 3.7 Evaluate on the test set\n","\n","We keep track of test_acc_rnn so the comparison section can place the CNN and RNN side by side.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"525fe860b7634c8da9a7cd54073aa201","outputId":"6c551a35-4b6b-43a7-e508-f7f8fa5d6640"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 406ms/step - accuracy: 0.8730 - loss: 0.3705\n","RNN test loss: 0.3696\n","RNN test accuracy: 0.8724\n"]}],"source":["\n","test_loss_rnn, test_acc_rnn = rnn_model.evaluate(test_ds)\n","print(f\"RNN test loss: {test_loss_rnn:.4f}\")\n","print(f\"RNN test accuracy: {test_acc_rnn:.4f}\")\n","# The accuracy is lower than the CNN, less time training , and make the task binary , and simple er"]},{"cell_type":"markdown","metadata":{"id":"d4162564b2ad4e9bbd40ce44af293c9a"},"source":["\n","### 3.8 Wrap the RNN for raw-text inference\n","\n","Just like we did for the CNN, we prepend the TextVectorization layer so the model accepts raw strings. These quick predictions double as a demo for practical usage.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3a15b1eec0884428a5353179a09f56cd"},"outputs":[],"source":["\n","rnn_inputs_str = tf.keras.Input(shape=(), dtype=tf.string, name=\"text\")\n","rnn_indices = vectorize_layer(rnn_inputs_str)\n","rnn_outputs_str = rnn_model(rnn_indices)\n","\n","rnn_end_to_end = tf.keras.Model(rnn_inputs_str, rnn_outputs_str, name=\"rnn_end_to_end\")\n","rnn_end_to_end.compile(\n","    loss=\"binary_crossentropy\",\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n","    metrics=[\"accuracy\"],\n",")\n","\n","sample_texts = [\n","    \"I absolutely loved this movie. The acting was fantastic and the story was touching.\",\n","    \"This was the worst film I have seen in years. Boring and predictable.\",\n","]\n","sample_texts_tensor = tf.constant(sample_texts, dtype=tf.string)\n","rnn_preds = rnn_end_to_end.predict(sample_texts_tensor)\n","for text, p in zip(sample_texts, rnn_preds):\n","    p_val = float(p.item())\n","    print(f\"[RNN] {text!r} -> positive probability = {p_val:.3f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"a8cdfb10e040475dbbf303eb215c34d5"},"source":["\n","## Part 4 – Compare the CNN and RNN\n","\n","Numbers alone rarely tell the whole story. This final section reuses the helper utilities to:\n","- print test accuracies side by side for both neural models,\n","- visualize loss/accuracy curves, and\n","- compare CNN vs. RNN predictions on the same sample reviews."]},{"cell_type":"markdown","metadata":{"id":"cf632359004048f58f433c768a9054f3"},"source":["\n","### 4.1 Summarize headline metrics (two models)\n","\n","We start by printing test metrics for the CNN and the BiLSTM so you have a quantitative snapshot before discussing dynamics or qualitative behavior.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0d1f10e1c202425fa53e93101ccb6793"},"outputs":[],"source":["print(f\"CNN test accuracy                       : {test_acc_cnn:.4f}\")\n","print(f\"RNN test accuracy                       : {test_acc_rnn:.4f}\")\n","\n","# Identify leader among the two\n","scores = {\n","    \"CNN\": test_acc_cnn,\n","    \"RNN\": test_acc_rnn,\n","}\n","leader_name = max(scores, key=scores.get)\n","leader_score = scores[leader_name]\n","runner_up = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[1]\n","print(f\"Leader: {leader_name} | accuracy: {leader_score:.4f} | gap: {leader_score - runner_up[1]:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"b0be09a080ab4a27a3aee7bc0b6b8d56"},"source":["\n","### 4.2 Compare training dynamics visually"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6ace0e294c741e0ae2ae4aa6526a13c"},"outputs":[],"source":["\n","plot_history(history_cnn, \"CNN\")\n","plot_history(history_rnn, \"RNN (BiLSTM)\")\n"]},{"cell_type":"markdown","metadata":{"id":"8bbd2f955ba340c2aaa9650d74875890"},"source":["\n","### 4.3 Qualitative predictions on the same reviews\n","\n","Numbers alone rarely reveal *how* the models disagree. We therefore feed the same reviews to each end-to-end model and print the predicted positive probabilities.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe57f2c973a04e5497d3855062d65026"},"outputs":[],"source":["comparison_texts = [\n","    \"The movie started off slow but eventually became very emotional and powerful.\",\n","    \"I fell asleep in the middle. The plot was dull and the acting terrible.\",\n","    \"Not bad, but not great either. Some scenes were really good, others felt pointless.\",\n","]\n","comparison_tensor = tf.constant(comparison_texts, dtype=tf.string)\n","\n","cnn_probs = cnn_end_to_end.predict(comparison_tensor)\n","rnn_probs = rnn_end_to_end.predict(comparison_tensor)\n","\n","print(\"\\n======================================\")\n","print(\" Sample prediction comparison (0–1)  \")\n","print(\"======================================\")\n","for text, pc, pr in zip(comparison_texts, cnn_probs, rnn_probs):\n","    print(f\"Review: {text}\")\n","    print(f\"  CNN positive probability: {float(pc):.3f}\")\n","    print(f\"  RNN positive probability: {float(pr):.3f}\")\n","    print(\"-\" * 60)\n"]},{"cell_type":"markdown","metadata":{"id":"b822581c914c40d795cc53cb5df9c789"},"source":["\n","### 4.4 Discussions\n","\n","- Which architecture gives higher validation accuracy? Does the difference persist on the test set?\n","- Do the learning curves reveal overfitting or faster convergence for one model?\n","- When predictions diverge, is it because of long-range structure (favoring RNNs) or local phrases (favoring CNNs)?"]},{"cell_type":"markdown","metadata":{"id":"82e02719bab84c7bbdbeca2676b91d7f"},"source":["#### Takeaways and next steps\n","\n","- We built a full NLP pipeline: raw IMDB text → custom preprocessing → reusable datasets that feed directly into neural architectures.\n","- Two complementary models (CNN and BiLSTM).\n","- Try extending the analysis by changing the vocabulary size, adjusting sequence length, adding pretrained embeddings, or experimenting with transformers to see how the comparison evolves. See https://keras.io/examples/nlp/text_classification_with_transformer/.\n"]},{"cell_type":"markdown","metadata":{"id":"ZF6DSpQd0xDi"},"source":[]}],"metadata":{"colab":{"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":0}